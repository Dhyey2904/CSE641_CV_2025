# -*- coding: utf-8 -*-
"""coatnet-0-inference-flops.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UqcUpqMSjsIYigTWTy1qTje4cTD3QMjU
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
hsankesara_flickr_image_dataset_path = kagglehub.dataset_download('hsankesara/flickr-image-dataset')

print('Data source import complete.')
print(hsankesara_flickr_image_dataset_path)

!pip install -q timm

pip install albumentations

import os
import cv2
import gc
import numpy as np
import pandas as pd
import itertools
from tqdm.autonotebook import tqdm
import albumentations as A
import matplotlib.pyplot as plt

import torch
from torch import nn
import torch.nn.functional as F
import timm
from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer

df = pd.read_csv("/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv", delimiter="|")
df.columns = ['image', 'caption_number', 'caption']
df['caption'] = df['caption'].str.lstrip()
df['caption_number'] = df['caption_number'].str.lstrip()
df.loc[19999, 'caption_number'] = "4"
df.loc[19999, 'caption'] = "A dog runs across the grass ."
ids = [id_ for id_ in range(len(df) // 5) for i in range(5)]
df['id'] = ids
df.to_csv("captions.csv", index=False)

df.head(10)

import torch

class CFG:
    debug = False
    image_path = "/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images"
    captions_path = "/kaggle/working/captions.csv"

    # Training parameters
    batch_size = 16          # Suitable for CoAtNet-0 on a GPU with moderate memory
    num_workers = 4          # Reasonable for Kaggle environment
    head_lr = 1e-3           # Learning rate for projection heads
    image_encoder_lr = 5e-5  # Slightly lower for fine-tuning CoAtNet-0 (pretrained)
    text_encoder_lr = 1e-5   # Suitable for fine-tuning DistilBERT
    weight_decay = 1e-5      # Small weight decay to prevent overfitting
    patience = 1             # For LR scheduler
    factor = 0.8             # LR reduction factor
    epochs = 10              # Reasonable for initial training
    learning_rate = 0.005    # General LR (not used directly with separate LRs)
    early_stopping_patience = 3  # Early stopping patience

    # Hardware configuration
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Model selection
    model_name = "coatnet_0_rw_224"  # CoAtNet-0 model (unchanged)
    image_embedding = 1024           # CoAtNet-0 feature dimension (confirmed)
    text_encoder_model = "distilbert-base-uncased"
    text_embedding = 768             # DistilBERT hidden size (unchanged)
    text_tokenizer = "distilbert-base-uncased"
    max_length = 200                 # Max token length for text (unchanged)

    # Pretrained settings
    pretrained = True   # Use pretrained CoAtNet-0 weights from timm
    trainable = True    # Allow fine-tuning of CoAtNet-0
    temperature = 1.0   # Initial temperature for CLIP (learnable later)

    # Image settings
    size = 224          # CoAtNet-0 expects 224x224 input images (unchanged)

    # Projection head settings
    num_projection_layers = 1  # Single projection layer (unchanged)
    projection_dim = 512       # Common projection dimension for image and text embeddings
    dropout = 0.1              # Dropout rate in projection head (unchanged)

    # CSV settings
    csv_delimiter = '|'  # Pipe delimiter as specified (unchanged)

cfg = CFG()

class AvgMeter:
    def __init__(self, name="Metric"):
        self.name = name
        self.reset()

    def reset(self):
        self.avg, self.sum, self.count = [0] * 3

    def update(self, val, count=1):
        self.count += count
        self.sum += val * count
        self.avg = self.sum / self.count

    def __repr__(self):
        text = f"{self.name}: {self.avg:.4f}"
        return text

def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group["lr"]

import torch
import cv2
import albumentations as A
from albumentations.pytorch import ToTensorV2

class CLIPDataset(torch.utils.data.Dataset):
    def __init__(self, image_filenames, captions, tokenizer, transforms):
        self.image_filenames = image_filenames
        self.captions = captions.astype(str).tolist()  # Ensure list of strings

        # Debugging: Print type of captions
        print("Caption Data Type:", type(self.captions))
        print("Example Caption:", self.captions[:5])  # Print first 5 captions

        self.encoded_captions = tokenizer(
            self.captions,  # Ensure it's a list of strings
            padding=True,
            truncation=True,
            max_length=CFG.max_length,
            return_tensors="pt",
        )
        self.transforms = transforms

    def __getitem__(self, idx):
        # Prepare tokenized text
        item = {key: values[idx] for key, values in self.encoded_captions.items()}

        # Load and preprocess image for CoAtNet-0
        image_path = f"{cfg.image_path}/{self.image_filenames[idx]}"
        image = cv2.imread(image_path)
        if image is None:
            raise FileNotFoundError(f"Image not found: {image_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = self.transforms(image=image)['image']  # Apply CoAtNet-0 compatible transforms

        item['image'] = image  # Already a PyTorch tensor from ToTensorV2
        item['caption'] = self.captions[idx]

        return item

    def __len__(self):
        return len(self.captions)


def get_transforms(mode="train"):
    """
    Define image transformations for CoAtNet-0 (224x224 input).
    Uses ImageNet normalization as CoAtNet-0 is pretrained on ImageNet.

    Args:
        mode (str): 'train' or 'valid'/'test'

    Returns:
        A.Compose: Albumentations transformation pipeline
    """
    imagenet_mean = [0.485, 0.456, 0.406]
    imagenet_std = [0.229, 0.224, 0.225]

    if mode == "train":
        return A.Compose([
            A.Resize(224, 224, always_apply=True),
            A.Normalize(mean=imagenet_mean, std=imagenet_std, max_pixel_value=255.0, always_apply=True),
            A.HorizontalFlip(p=0.5),  # Augmentation for training
            A.RandomBrightnessContrast(p=0.2),
            ToTensorV2(),  # Converts to PyTorch tensor (C, H, W)
        ])
    else:  # Validation/Test
        return A.Compose([
            A.Resize(224, 224, always_apply=True),
            A.Normalize(mean=imagenet_mean, std=imagenet_std, max_pixel_value=255.0, always_apply=True),
            ToTensorV2(),
        ])

import timm
import torch
import torch.nn as nn

class ImageEncoder(nn.Module):
    """
    Image Encoder based on CoAtNet-0 (coatnet_0_rw_224) from timm.
    Extracts 1024-dimensional feature embeddings from 224x224 RGB images.

    Args:
        model_name (str): Name of the CoAtNet model (default: 'coatnet_0_rw_224')
        pretrained (bool): Load pretrained weights (default: True)
        trainable (bool): Allow fine-tuning (default: True)
        freeze_layers (int): Number of initial layers to freeze (default: 0, meaning all layers trainable)
    """
    def __init__(self, model_name="coatnet_0_rw_224", pretrained=True, trainable=True, freeze_layers=0):
        super().__init__()

        # Initialize CoAtNet-0 without classification head
        self.model = timm.create_model(
            model_name,
            pretrained=pretrained,
            num_classes=0,       # Remove classification head
            global_pool="avg"    # Use average pooling to get 1024-d features
        )

        self.feature_dim = 1024  # CoAtNet-0 outputs 1024-dimensional features

        # Control model trainability
        if not trainable:
            for param in self.model.parameters():
                param.requires_grad = False
        else:
            # Freeze specific number of layers (if freeze_layers > 0)
            layers = list(self.model.children())
            for i, layer in enumerate(layers[:freeze_layers]):
                for param in layer.parameters():
                    param.requires_grad = False

    def forward(self, x):
        """
        Forward pass through CoAtNet-0.

        Args:
            x (torch.Tensor): Input images of shape [batch_size, 3, 224, 224]

        Returns:
            torch.Tensor: Feature embeddings of shape [batch_size, 1024]
        """
        return self.model(x)  # Output: [batch_size, 1024]

import torch
import torch.nn as nn
from transformers import DistilBertModel, DistilBertConfig

class TextEncoder(nn.Module):
    """
    Text Encoder based on DistilBERT for use with CoAtNet-0 in a CLIP-like model.
    Extracts 768-dimensional embeddings from text using the CLS token.

    Args:
        model_name (str): DistilBERT model name (default: 'distilbert-base-uncased')
        pretrained (bool): Load pretrained weights (default: True)
        trainable (bool): Allow fine-tuning (default: True)
        cls_token_idx (int): Index of CLS token in the last hidden state (default: 0)
    """
    def __init__(self, model_name="distilbert-base-uncased", pretrained=True, trainable=True, cls_token_idx=0):
        super().__init__()

        # Load DistilBERT model (pretrained or from scratch)
        if pretrained:
            self.model = DistilBertModel.from_pretrained(model_name)
        else:
            config = DistilBertConfig()
            self.model = DistilBertModel(config)

        # Make the model trainable or frozen
        for param in self.model.parameters():
            param.requires_grad = trainable

        # Feature size (DistilBERT hidden size)
        self.feature_dim = self.model.config.hidden_size  # Usually 768

        # CLS token index
        self.cls_token_idx = cls_token_idx

    def forward(self, input_ids, attention_mask):
        """
        Forward pass through DistilBERT to encode text.

        Args:
            input_ids (torch.Tensor): Tokenized text input IDs [batch_size, max_length]
            attention_mask (torch.Tensor): Attention mask [batch_size, max_length]

        Returns:
            torch.Tensor: CLS token embedding [batch_size, 768]
        """
        with torch.cuda.amp.autocast():  # Mixed precision support
            output = self.model(input_ids=input_ids, attention_mask=attention_mask)

        # Extract CLS token embedding
        cls_embedding = output.last_hidden_state[:, self.cls_token_idx, :]  # [batch_size, 768]

        return cls_embedding

import torch
import torch.nn as nn

class ProjectionHead(nn.Module):
    """
    Projection Head to map embeddings from CoAtNet-0 (1024) or DistilBERT (768) to a common
    lower-dimensional space (e.g., 512) for CLIP-style training.

    Args:
        embedding_dim (int): Input dimension (1024 for CoAtNet, 768 for DistilBERT).
        projection_dim (int): Output dimension (default: 512).
        dropout (float): Dropout rate (default: 0.1).
    """
    def __init__(self, embedding_dim, projection_dim=512, dropout=0.1):
        super().__init__()

        # Linear projection
        self.projection = nn.Linear(768, projection_dim)

        # Fully connected refinement layer
        self.fc = nn.Linear(projection_dim, projection_dim)

        # Activation function (GELU is commonly used in transformers)
        self.activation = nn.GELU()

        # Dropout for regularization
        self.dropout = nn.Dropout(dropout)

        # Layer normalization for stable training
        self.layer_norm = nn.LayerNorm(projection_dim)

        # Weight initialization (optional but improves convergence)
        nn.init.xavier_uniform_(self.projection.weight)
        nn.init.xavier_uniform_(self.fc.weight)
        nn.init.zeros_(self.projection.bias)
        nn.init.zeros_(self.fc.bias)

    def forward(self, x):
        """
        Forward pass through the projection head.

        Args:
            x (torch.Tensor): Input embeddings [batch_size, embedding_dim]
                              (e.g., [batch_size, 1024] from CoAtNet or [batch_size, 768] from DistilBERT)

        Returns:
            torch.Tensor: Projected embeddings [batch_size, projection_dim]
                          (e.g., [batch_size, 512])
        """
        with torch.cuda.amp.autocast():  # Enable mixed precision
            projected = self.projection(x)  # [batch_size, projection_dim]
            x = self.activation(projected)
            x = self.fc(x)
            x = self.dropout(x)

            # Apply residual connection if dimensions match
            if x.shape == projected.shape:
                x = x + projected

            x = self.layer_norm(x)

        return x

import torch
import torch.nn as nn
import torch.nn.functional as F

class CLIPModel(nn.Module):
    """
    CLIP-like model with an image encoder (CoAtNet-0), a text encoder (DistilBERT),
    and projection heads to align both in a common latent space.

    Args:
        temperature (float): Temperature parameter for contrastive learning.
    """
    def __init__(self, temperature=0.07):
        super().__init__()
        self.image_encoder = ImageEncoder()  # Outputs (batch, 1024)
        self.text_encoder = TextEncoder()  # Outputs (batch, 768)
        self.image_projection = ProjectionHead(embedding_dim=1024, projection_dim=512)
        self.text_projection = ProjectionHead(embedding_dim=768, projection_dim=512)

        # Learnable temperature parameter
        self.temperature = nn.Parameter(torch.tensor(temperature, dtype=torch.float32))

    def forward(self, batch):
        """
        Forward pass for CLIP-style training.

        Args:
            batch (dict): Dictionary containing:
                - "image" (torch.Tensor): Image batch of shape [batch_size, 3, 224, 224]
                - "input_ids" (torch.Tensor): Tokenized text input IDs [batch_size, max_length]
                - "attention_mask" (torch.Tensor): Attention masks [batch_size, max_length]

        Returns:
            torch.Tensor: Contrastive loss (scalar)
        """
        device = self.temperature.device  # Ensure all tensors are on the same device

        # Extract batch data
        images = batch["image"].to(device)
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        # Encode images and texts
        image_embeddings = self.image_encoder(images)  # (batch, 1024)
        text_embeddings = self.text_encoder(input_ids, attention_mask)  # (batch, 768)

        # Project embeddings to a common latent space
        image_features = F.normalize(self.image_projection(image_embeddings), dim=-1)  # (batch, 512)
        text_features = F.normalize(self.text_projection(text_embeddings), dim=-1)  # (batch, 512)

        # Compute similarity matrix
        logits = (text_features @ image_features.T) / self.temperature  # (batch, batch)

        # Compute self-similarities for soft target computation
        images_similarity = image_features @ image_features.T  # (batch, batch)
        texts_similarity = text_features @ text_features.T  # (batch, batch)

        # Compute soft targets
        targets = F.softmax((images_similarity + texts_similarity) / 2 * self.temperature, dim=-1)

        # Compute contrastive loss (corrected)
        texts_loss = F.cross_entropy(logits, targets.argmax(dim=-1), reduction="mean")
        images_loss = F.cross_entropy(logits.T, targets.argmax(dim=-1), reduction="mean")

        loss = (texts_loss + images_loss) / 2.0

        return loss

import pandas as pd
import numpy as np
import torch
from torch.utils.data import DataLoader

def make_train_valid_dfs():
    dataframe = pd.read_csv(CFG.captions_path)

    # Drop missing captions
    dataframe = dataframe.dropna(subset=["caption"]).reset_index(drop=True)

    # Convert captions to string
    dataframe["caption"] = dataframe["caption"].astype(str)

    max_id = dataframe["id"].max() + 1 if not CFG.debug else 100
    image_ids = np.arange(0, max_id)

    np.random.seed(42)
    valid_ids = np.random.choice(image_ids, size=int(0.2 * len(image_ids)), replace=False)
    train_ids = np.setdiff1d(image_ids, valid_ids)

    train_dataframe = dataframe[dataframe["id"].isin(train_ids)].reset_index(drop=True)
    valid_dataframe = dataframe[dataframe["id"].isin(valid_ids)].reset_index(drop=True)

    return train_dataframe, valid_dataframe



def build_loaders(dataframe, tokenizer, mode, batch_size=32, num_workers=4):
    """
    Builds DataLoader for training and validation.

    Args:
        dataframe (pd.DataFrame): Data containing image paths and captions.
        tokenizer (transformers.PreTrainedTokenizer): Tokenizer for text processing.
        mode (str): "train" or "valid" to define dataset mode.
        batch_size (int): Batch size for DataLoader.
        num_workers (int): Number of workers for data loading.

    Returns:
        torch.utils.data.DataLoader: DataLoader for the given mode.
    """
    transforms = get_transforms(mode=mode)  # Ensure compatibility with CoAtNet-0

    dataset = CLIPDataset(
        dataframe["image"].values,
        dataframe["caption"].values,
        tokenizer=tokenizer,
        transforms=transforms,
    )

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=(mode == "train"),
        pin_memory=True
    )

    return dataloader

import torch
from tqdm import tqdm

def get_lr(optimizer):
    """
    Gets the current learning rate from the optimizer.

    Args:
        optimizer (torch.optim.Optimizer): The optimizer.

    Returns:
        float: Current learning rate.
    """
    return optimizer.param_groups[0]["lr"]

def train_epoch(model, train_loader, optimizer, lr_scheduler, step, device="cuda"):
    """
    Trains the model for one epoch.

    Args:
        model (nn.Module): The CLIP model.
        train_loader (DataLoader): Training data loader.
        optimizer (torch.optim.Optimizer): Optimizer for updating weights.
        lr_scheduler (torch.optim.lr_scheduler._LRScheduler): Learning rate scheduler.
        step (str): "batch" or "epoch" step for learning rate scheduling.
        device (str): Device to run training on ("cuda" or "cpu").

    Returns:
        AvgMeter: Average training loss.
    """
    model.train()
    loss_meter = AvgMeter()
    tqdm_object = tqdm(train_loader, total=len(train_loader), desc="Training")

    for batch in tqdm_object:
        batch = {k: v.to(device, non_blocking=(device == "cuda")) for k, v in batch.items() if k != "caption"}

        optimizer.zero_grad(set_to_none=True)
        loss = model(batch)

        loss.backward()
        optimizer.step()

        if step == "batch":
            lr_scheduler.step()
        elif step != "epoch":
            raise ValueError("Invalid step type. Choose 'batch' or 'epoch'.")

        count = batch["image"].size(0)
        loss_meter.update(loss.item(), count)

        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))

    return loss_meter


def valid_epoch(model, valid_loader, device="cuda"):
    """
    Validates the model for one epoch.

    Args:
        model (nn.Module): The CLIP model.
        valid_loader (DataLoader): Validation data loader.
        device (str): Device to run validation on ("cuda" or "cpu").

    Returns:
        AvgMeter: Average validation loss.
    """
    model.eval()
    loss_meter = AvgMeter()
    tqdm_object = tqdm(valid_loader, total=len(valid_loader), desc="Validation")

    with torch.no_grad():
        for batch in tqdm_object:
            batch = {k: v.to(device, non_blocking=(device == "cuda")) for k, v in batch.items() if k != "caption"}

            loss = model(batch)

            count = batch["image"].size(0)
            loss_meter.update(loss.item(), count)

            tqdm_object.set_postfix(valid_loss=loss_meter.avg)

    return loss_meter

import torch
import itertools
from transformers import DistilBertTokenizer

# Load DataFrames
train_df, valid_df = make_train_valid_dfs()

# Load Tokenizer
tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)

# Create Data Loaders
train_loader = build_loaders(train_df, tokenizer, mode="train")
valid_loader = build_loaders(valid_df, tokenizer, mode="valid")

# Initialize Model with CoAtNet-0
model = CLIPModel().to(CFG.device)

# Optimizer Parameters (Separate LRs for Image Encoder, Text Encoder, and Projection Heads)
params = [
    {"params": model.image_encoder.parameters(), "lr": CFG.image_encoder_lr},
    {"params": model.text_encoder.parameters(), "lr": CFG.text_encoder_lr},
    {"params": itertools.chain(
        model.image_projection.parameters(), model.text_projection.parameters()
    ), "lr": CFG.head_lr, "weight_decay": CFG.weight_decay}
]

# AdamW Optimizer
optimizer = torch.optim.AdamW(params, weight_decay=CFG.weight_decay)

# Learning Rate Scheduler (Reduce LR on Plateau)
lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode="min", patience=CFG.patience, factor=CFG.factor
)

# Early Stopping Setup
best_loss = float('inf')
early_stopping_counter = 0
early_stopping_patience = CFG.early_stopping_patience  # Define in config

# Training Loop
for epoch in range(CFG.epochs):
    print(f"\nüîπ Epoch {epoch + 1}/{CFG.epochs}")

    # Train Model
    train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step="epoch")

    # Validate Model
    with torch.no_grad():
        valid_loss = valid_epoch(model, valid_loader)

    # Print Losses
    print(f"üìâ Training Loss: {train_loss.avg:.4f} | üèÅ Validation Loss: {valid_loss.avg:.4f}")

    # Save Best Model
    if valid_loss.avg < best_loss:
        best_loss = valid_loss.avg
        early_stopping_counter = 0
        torch.save(model.state_dict(), "best_coatnet0.pt")
        print("‚úÖ Saved Best Model!")
    else:
        early_stopping_counter += 1

    # Step LR Scheduler (Ensure Float Conversion)
    lr_scheduler.step(float(valid_loss.avg))

    # Early Stopping
    if early_stopping_counter >= early_stopping_patience:
        print(f"‚èπÔ∏è Early stopping triggered after {early_stopping_patience} epochs.")
        break

import torch
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
from transformers import DistilBertTokenizer

# Load trained model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CLIPModel().to(device)
model.load_state_dict(torch.load("/kaggle/input/dataset2/best_coatnet0.pt", map_location=device))
model.eval()

# Initialize tokenizer
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")

def extract_features(dataloader, model, feature_type="image"):
    """Extract normalized image or text features from the model."""
    features = []
    with torch.no_grad():
        for batch in tqdm(dataloader, desc=f"Extracting {feature_type} features"):
            batch = {k: v.to(device) for k, v in batch.items() if k != "caption"}

            if feature_type == "image":
                feature = model.image_projection(model.image_encoder(batch["image"]))
            else:
                feature = model.text_projection(model.text_encoder(
                    input_ids=batch["input_ids"], attention_mask=batch["attention_mask"]
                ))

            features.append(F.normalize(feature, dim=1).cpu().numpy())
    return np.vstack(features)

# Load validation dataset
_, valid_dataframe = make_train_valid_dfs()
valid_loader = build_loaders(valid_dataframe, tokenizer, mode="valid")

# Extract image and text features
image_features = extract_features(valid_loader, model, "image")
text_features = extract_features(valid_loader, model, "text")

# Compute similarity matrix
similarity_matrix = np.matmul(text_features, image_features.T)

def compute_retrieval_metrics(similarity_matrix, valid_df, top_k=[1, 5, 10]):
    """Compute Recall@K and Mean Average Precision (mAP)."""
    num_queries = similarity_matrix.shape[0]
    recall_at_k = {k: 0 for k in top_k}
    average_precision = []

    image_to_indices = valid_df.groupby("image").indices

    for i in range(num_queries):
        sorted_indices = np.argsort(similarity_matrix[i])[::-1]
        gt_image = valid_df.iloc[i]["image"]
        gt_indices = image_to_indices[gt_image]

        for k in top_k:
            if any(idx in sorted_indices[:k] for idx in gt_indices):
                recall_at_k[k] += 1

        ranks = [np.where(sorted_indices == idx)[0][0] + 1 for idx in gt_indices]
        average_precision.append(np.mean([1.0 / rank for rank in ranks]))

    recall_at_k = {k: recall_at_k[k] / num_queries for k in recall_at_k}
    mean_ap = np.mean(average_precision)
    return recall_at_k, mean_ap

# Compute and display metrics
recall_k, mean_ap = compute_retrieval_metrics(similarity_matrix, valid_dataframe)
print("\nRetrieval Performance:")
for k in recall_k:
    print(f"Recall@{k}: {recall_k[k]:.4f}")
print(f"Mean Average Precision (mAP): {mean_ap:.4f}")

def get_image_embeddings(valid_df, model_path):
    tokenizer = DistilBertTokenizer.from_pretrained(cfg.text_tokenizer)
    valid_loader = build_loaders(valid_df, tokenizer, mode="valid")

    model = CLIPModel().to(cfg.device)
    model.load_state_dict(torch.load(model_path, map_location=cfg.device))
    model.eval()

    valid_image_embeddings = []
    with torch.no_grad():
        for batch in tqdm(valid_loader):
            image_features = model.image_encoder(batch["image"].to(cfg.device))
            image_embeddings = model.image_projection(image_features)
            valid_image_embeddings.append(image_embeddings)
    return model, torch.cat(valid_image_embeddings)

_, valid_df = make_train_valid_dfs()
model, image_embeddings = get_image_embeddings(valid_df, "/kaggle/input/dataset2/best_coatnet0.pt")

def find_matches(model, image_embeddings, query, image_filenames, n=9):
    tokenizer = DistilBertTokenizer.from_pretrained(cfg.text_tokenizer)
    encoded_query = tokenizer([query])
    batch = {
        key: torch.tensor(values).to(cfg.device)
        for key, values in encoded_query.items()
    }
    with torch.no_grad():
        text_features = model.text_encoder(
            input_ids=batch["input_ids"], attention_mask=batch["attention_mask"]
        )
        text_embeddings = model.text_projection(text_features)

    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)
    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)
    dot_similarity = text_embeddings_n @ image_embeddings_n.T

    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)
    matches = [image_filenames[idx] for idx in indices[::5]]

    _, axes = plt.subplots(3, 3, figsize=(10, 10))
    for match, ax in zip(matches, axes.flatten()):
        image = cv2.imread(f"{cfg.image_path}/{match}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        ax.imshow(image)
        ax.axis("off")

    plt.show()

find_matches(model,
             image_embeddings,
             query="one dog sitting on the grass",
             image_filenames=valid_df['image'].values,
             n=9)

import time
# Inference timing example
start_time = time.time()
with torch.no_grad():
    find_matches(model,
             image_embeddings,
             query="Two young guys with shaggy hair look at their hands while hanging out in the yard",
             image_filenames=valid_df['image'].values,
             n=9)  # Replace `batch` with your actual inference input
end_time = time.time()
print(f"üïí Inference Time: {end_time - start_time:.4f} seconds")

!pip install thop

# Make sure thop is installed
# !pip install thop
from thop import profile, clever_format

# Dummy input matching the model signature
dummy_image = torch.randn(1, 3, cfg.size, cfg.size).to(cfg.device)
dummy_input_ids = torch.randint(0, 30522, (1, cfg.max_length)).to(cfg.device)
dummy_attention_mask = torch.ones((1, cfg.max_length)).to(cfg.device)

dummy_batch = {
    "image": dummy_image,
    "input_ids": dummy_input_ids,
    "attention_mask": dummy_attention_mask
}

# Calculate FLOPs and Params
flops, params = profile(model, inputs=(dummy_batch,), verbose=False)
flops, params = clever_format([flops, params], "%.3f")
print(f"üî¢ FLOPs: {flops} | üì¶ Parameters: {params}")

import time
queries = [
    "A dog sitting on the grass",
    "A man riding a bicycle",
    "A woman walking in the park",
    "Two kids playing soccer",
    "A cat sleeping on a sofa"
]

start_time = time.time()
with torch.no_grad():
    for q in queries:
        find_matches(
            model,
            image_embeddings,
            query=q,
            image_filenames=valid_df['image'].values,
            n=9
        )
end_time = time.time()
print(f"üïí Total Inference Time for {len(queries)} queries: {end_time - start_time:.4f} seconds")
print(f"‚è±Ô∏è Average per query: {(end_time - start_time) / len(queries):.4f} seconds")

