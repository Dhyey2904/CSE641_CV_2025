# -*- coding: utf-8 -*-
"""efficiientnetb3-flops-inference (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cz2pseSGN_wikMr_jSPv1PoeSsxDVPOs
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
hsankesara_flickr_image_dataset_path = kagglehub.dataset_download('hsankesara/flickr-image-dataset')

print('Data source import complete.')
print(hsankesara_flickr_image_dataset_path)

"""# CLIP from Scratch


**CLIP** or **Contrastive Language-Image Pre-training** is a model that learns the relationship between a whole sentence and the image it describes; in a sense that when the model is trained, given an input sentence it will be able to retrieve the most related images corresponding to that sentence. The important thing here is that it is trained on full sentences instead of single classes like car, dog, etc. The intuition is that when trained on whole sentences, the model can learn a lot more things and finds some pattern between images and texts.
They also show that when this model is trained on a huge dataset of images and their corresponding texts, it can also act as a classifier too. I encourage you to study the paper to learn more about this exciting model and their astonishing results on benchmarking datasetsÂ . To mention just one, CLIP model trained with this strategy classifies ImageNet better than those SOTA models trained on the ImageNet itself optimized for the only task of classification!

As a **teaser**, let's see what the final model that we will build in this article from scratch is capable of: given a query (raw text) like "a boy jumping with skateboard" or "a girl jumping from swing", the model will retrieve the most relevant images:

![](https://i.ibb.co/9gdYqNP/teaser-cropped.png)

In this notebook, we will see how to implement CLIP from Scratch

## Imports
"""

!pip install -q timm

pip install albumentations

import os
import cv2
import gc
import numpy as np
import pandas as pd
import itertools
from tqdm.autonotebook import tqdm
import albumentations as A
import matplotlib.pyplot as plt

import torch
from torch import nn
import torch.nn.functional as F
import timm
from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer

"""## Some pre-preocessing"""

df = pd.read_csv("/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv", delimiter="|")
df.columns = ['image', 'caption_number', 'caption']
df['caption'] = df['caption'].str.lstrip()
df['caption_number'] = df['caption_number'].str.lstrip()
df.loc[19999, 'caption_number'] = "4"
df.loc[19999, 'caption'] = "A dog runs across the grass ."
ids = [id_ for id_ in range(len(df) // 5) for i in range(5)]
df['id'] = ids
df.to_csv("captions.csv", index=False)

df.head(10)

"""## Config"""

class CFG:
    debug = False
    image_path = "/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images"
    captions_path = "/kaggle/working/captions.csv"

    # Training parameters
    batch_size = 16
    num_workers = 4
    head_lr = 1e-3
    image_encoder_lr = 1e-4
    text_encoder_lr = 1e-5
    weight_decay = 1e-5
    patience = 1
    factor = 0.8
    epochs = 10
    learning_rate = 0.005
    early_stopping_patience = 3

    # Hardware configuration
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Model selection
    model_name = "efficientnet_b3"  # EfficientNet-B3 for better accuracy vs. compute balance
    image_embedding = 1536  # EfficientNet-B3 feature dimension
    text_encoder_model = "distilbert-base-uncased"
    text_embedding = 768
    text_tokenizer = "distilbert-base-uncased"
    max_length = 200

    # Pretrained settings
    pretrained = True  # Use pretrained weights for both encoders
    trainable = True  # Allow fine-tuning of both encoders
    temperature = 1.0

    # Image settings
    size = 300  # Recommended input size for EfficientNet-B3

    # Projection head settings (for image and text encoders)
    num_projection_layers = 1
    projection_dim = 512  # Projection dimension matching embeddings
    dropout = 0.1

    # CSV settings
    csv_delimiter = '|'  # Pipe delimiter as specified

cfg = CFG()

"""## Utils"""

class AvgMeter:
    def __init__(self, name="Metric"):
        self.name = name
        self.reset()

    def reset(self):
        self.avg, self.sum, self.count = [0] * 3

    def update(self, val, count=1):
        self.count += count
        self.sum += val * count
        self.avg = self.sum / self.count

    def __repr__(self):
        text = f"{self.name}: {self.avg:.4f}"
        return text

def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group["lr"]

"""## Dataset

We need to encode both images and their describing texts. We use **Flickr 30k** dataset that contains 31.8k images and caption pairs.

We will use **DistilBERT** model (which is smaller than BERT but performs nearly as well as BERT) from **HuggingFace** library as our text encoder; so, we need to **tokenize** the sentences (captions) with DistilBERT tokenizer and then feed the token ids (input_ids) and the attention masks to DistilBERT. Therefore, the dataset needs to take care of the tokenization as well. Below you can see the dataset's code. Below that I'll explain the most important things that is happening in the code.

In the **\_\_init\_\_** we receive a tokenizer object which is actually a HuggingFace tokinzer; this tokenizer will be loaded when running the model. We are padding and truncating the captions to a specified max_length. In the **\_\_getitem\_\_** we will first load an encoded caption which is a dictionary with keys input_ids and attention_mask, make tensors out of its values and after that we will load the corresponding image, transform and augment it (if there is any!) and then we make it a tensor and put it in the dictionary with "image" as the key. Finally we put the raw text of the caption with the key "caption" in the dictionary only for visualization purposes.

I did not use additional data augmentations but you can add them if you want to improve the model's performance.
"""

class CLIPDataset(torch.utils.data.Dataset):
    def __init__(self, image_filenames, captions, tokenizer, transforms):
        """
        image_filenames and cpations must have the same length; so, if there are
        multiple captions for each image, the image_filenames must have repetitive
        file names
        """

        self.image_filenames = image_filenames
        self.captions = list(captions)
        self.encoded_captions = tokenizer(
            list(captions), padding=True, truncation=True, max_length=cfg.max_length
        )
        self.transforms = transforms

    def __getitem__(self, idx):
        item = {
            key: torch.tensor(values[idx])
            for key, values in self.encoded_captions.items()
        }

        image = cv2.imread(f"{cfg.image_path}/{self.image_filenames[idx]}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = self.transforms(image=image)['image']
        item['image'] = torch.tensor(image).permute(2, 0, 1).float()
        item['caption'] = self.captions[idx]

        return item


    def __len__(self):
        return len(self.captions)



def get_transforms(mode="train"):
    if mode == "train":
        return A.Compose(
            [
                A.Resize(cfg.size, cfg.size, always_apply=True),
                A.Normalize(max_pixel_value=255.0, always_apply=True),
            ]
        )
    else:
        return A.Compose(
            [
                A.Resize(cfg.size, cfg.size, always_apply=True),
                A.Normalize(max_pixel_value=255.0, always_apply=True),
            ]
        )

"""## Image Encoder

The image encoder code is straight forward. I'm using PyTorch Image Models library (timm) here which makes a lot of different image models available from ResNets to EfficientNets and many more. Here we will use a ResNet50 as our image encoder. You can easily use torchvision library to use ResNets if you don't want to install a new library.

The code encodes each image to a fixed size vector with the size of the model's output channels (in case of ResNet50 the vector size will be **2048**). This is the output after the nn.AdaptiveAvgPool2d() layer.
"""

import timm
import torch.nn as nn

class ImageEncoder(nn.Module):
    """
    EfficientNet-B3 based Image Encoder
    """

    def __init__(
        self, model_name=cfg.model_name, pretrained=cfg.pretrained, trainable=cfg.trainable
    ):
        super().__init__()
        self.model = timm.create_model(
            model_name, pretrained=pretrained, num_classes=0, global_pool="avg"
        )

        # EfficientNet-B3 outputs 1536 feature dimensions
        self.feature_dim = 1536

        # Freeze/Unfreeze model parameters based on trainable flag
        if not trainable:
            for param in self.model.parameters():
                param.requires_grad = False

    def forward(self, x):
        x = self.model(x)  # Extract features
        return x  # Output shape: (batch_size, 1536)

"""## Text Encoder

I'll use DistilBERT as the text encoder. Like its bigger brother BERT, two special tokens will be added to the actual input tokens: **CLS** and **SEP** which mark the start and end of a sentence. To grab the whole representation of a sentence (as the related BERT and DistilBERT papers point out) we use the final representations of the CLS token and we hope that this representation captures the overall meaning of the sentence (caption). Thinking it in this way, it is similar to what we did to images and converted them into a fixed size vector.

In the case of DistilBERT (and also BERT) the output hidden representation for each token is a vector with size **768**. So, the whole caption will be encoded in the CLS token representation whose size is 768.
"""

import torch.nn as nn
from transformers import DistilBertModel, DistilBertConfig

class TextEncoder(nn.Module):
    """
    DistilBERT-based Text Encoder
    """

    def __init__(self, model_name=cfg.text_encoder_model, pretrained=cfg.pretrained, trainable=cfg.trainable):
        super().__init__()

        # Load DistilBERT model with or without pretrained weights
        if pretrained:
            self.model = DistilBertModel.from_pretrained(model_name)
        else:
            config = DistilBertConfig()
            self.model = DistilBertModel(config)

        # Set requires_grad based on the trainable flag
        if not trainable:
            for param in self.model.parameters():
                param.requires_grad = False

        # CLS token index for extracting sentence embedding
        self.cls_token_idx = 0

    def forward(self, input_ids, attention_mask):
        """
        Forward pass for text encoding.

        Args:
            input_ids (torch.Tensor): Tokenized text input IDs.
            attention_mask (torch.Tensor): Attention mask for input text.

        Returns:
            torch.Tensor: Sentence embedding from CLS token.
        """
        output = self.model(input_ids=input_ids, attention_mask=attention_mask)
        return output.last_hidden_state[:, self.cls_token_idx, :]  # Extract CLS token representation

"""## Projection Head

Now that we have encoded both our images and texts into fixed size vectors (2048 for image and 768 for text) we need to bring (project) them into a _new world_ with **similar dimensions** for both images and texts in order to be able to compare them and push apart the non-relevant image and texts and pull together those that match. So, the following code will bring the 2048 and 768 dimensional vectors into a 256 (projection_dim) dimensional world, where we can **compare** them.

"embedding_dim" is the size of the input vector (2048 for images and 768 for texts) and "projection_dim" is the the size of the output vector which will be 256 for our case. For understanding the details of this part you can refer to the CLIP paper.
"""

import torch.nn as nn

class ProjectionHead(nn.Module):
    """
    Projection Head to map embeddings to a lower-dimensional space.
    """

    def __init__(self, embedding_dim, projection_dim=cfg.projection_dim, dropout=cfg.dropout):
        super().__init__()

        self.projection = nn.Linear(embedding_dim, projection_dim)
        self.activation = nn.GELU()
        self.fc = nn.Linear(projection_dim, projection_dim)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(projection_dim)

    def forward(self, x):
        """
        Forward pass for projection head.

        Args:
            x (torch.Tensor): Input feature embeddings.

        Returns:
            torch.Tensor: Projected feature embeddings.
        """
        projected = self.projection(x)
        x = self.activation(projected)
        x = self.fc(x)
        x = self.dropout(x)
        x = x + projected  # Residual connection
        return self.layer_norm(x)  # Normalize output

"""## CLIP

![clip.png](attachment:fb0403a4-73b2-4b97-bfec-c824d11677ee.png)

Here we will use the previous modules that we built to implement the main model. The \_\_init\_\_ function is self-explanatory. In the forward function, we first encode the images and texts separately into fixed size vectors (with different dimensionalities). After that, using separate projection modules we project them to that shared world (space) that I talked about previously. Here the encodings will become of similar shape (256 in our case). After that we will compute the loss. Again I recommend reading CLIP paper to get it better but I'll try my best to explain this part.

In Linear Algebra, one common way to measure if two vectors are of similar characteristics (they are like each other) is to calculate their **dot product** (multiplying the matching entries and take the sum of them); if the final number is big, they are alike and if it is small they are not (relatively speaking)!

Let's now understand the loss function. We talked about two vectors, but, what do we have here? We have image_embeddings, a matrix with shape (batch_size, 256) and text_embeddings with shape (batch_size, 256). It means we have two groups of vectors instead of two single vectors. How do we measure how similar two groups of vectors (two matrices) are to each other? Again, with dot product (@ operator in PyTorch does the dot product or matrix multiplication in this case). To be able to multiply these two matrices together, we transpose the second one. Okay, we get a matrix with shape (batch_size, batch_size) which we will call logits. (temperature is equal to 1.0 in our case, so, it does not make a difference. You can play with it and see what difference it makes. Also look at the paper to see why it is here!).
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import DistilBertModel
import timm

class CLIPModel(nn.Module):
    def __init__(
        self,
        temperature=cfg.temperature,
        image_embedding=cfg.image_embedding,  # 1280 for EfficientNet-B3
        text_embedding=cfg.text_embedding,    # 768 for DistilBERT
    ):
        super().__init__()

        # Image Encoder (EfficientNet-B3)
        self.image_encoder = ImageEncoder()

        # Text Encoder (DistilBERT)
        self.text_encoder = TextEncoder()

        # Projection Heads
        self.image_projection = ProjectionHead(embedding_dim=image_embedding)
        self.text_projection = ProjectionHead(embedding_dim=text_embedding)

        # Temperature parameter for contrastive loss
        self.temperature = nn.Parameter(torch.tensor(temperature))

    def forward(self, batch):
        """
        Forward pass for CLIP Model.

        Args:
            batch (dict): Dictionary containing images and tokenized text.

        Returns:
            torch.Tensor: Contrastive loss.
        """
        # Encode Image and Text Features
        image_features = self.image_encoder(batch["image"])
        text_features = self.text_encoder(
            input_ids=batch["input_ids"], attention_mask=batch["attention_mask"]
        )

        # Project to shared embedding space
        image_embeddings = self.image_projection(image_features)
        text_embeddings = self.text_projection(text_features)

        # Compute logits (cosine similarity scaled by temperature)
        logits = (text_embeddings @ image_embeddings.T) / self.temperature

        # Compute similarity matrices
        images_similarity = image_embeddings @ image_embeddings.T
        texts_similarity = text_embeddings @ text_embeddings.T

        # Target similarity distribution
        targets = F.softmax((images_similarity + texts_similarity) / 2 * self.temperature, dim=-1)

        # Contrastive loss
        texts_loss = cross_entropy(logits, targets, reduction='none')
        images_loss = cross_entropy(logits.T, targets.T, reduction='none')

        # Final loss (average over batch)
        loss = (images_loss + texts_loss) / 2.0
        return loss.mean()


def cross_entropy(preds, targets, reduction='none'):
    """
    Computes cross-entropy loss.

    Args:
        preds (torch.Tensor): Logits from model.
        targets (torch.Tensor): Softmax target distribution.
        reduction (str): Loss reduction method ('none' or 'mean').

    Returns:
        torch.Tensor: Computed loss.
    """
    log_softmax = nn.LogSoftmax(dim=-1)
    loss = (-targets * log_softmax(preds)).sum(1)
    return loss if reduction == "none" else loss.mean()

"""So, in the best case scenario, text_embeddings and image_embedding matricies should be the same because they are describing similar things. Let's think now: if this happens, what would the logits matrix be like? Let's see with a simple example!"""

# A simple Example

batch_size = 4
dim = 256
embeddings = torch.randn(batch_size, dim)
out = embeddings @ embeddings.T
print(F.softmax(out, dim=-1))

"""So logits, in the best case, will be a matrix that if we take its softmax, will have 1.0s in the diagonal (An identity matrix to call it with fancy words!). As the loss function's job is to make model's predictions similar to targets (at least in most cases!), we want such a matrix as our target. That's the reason why we are calculating images_similarity and texts_similarity matrices in the code block above.

Now that we've got our targets matrix, we will use simple cross entropy to calculate the actual loss. I've written the full matrix form of cross entropy as a function which you can see in the bottom of the code block.

There's a simpler way to calculate this loss in PyTorch; by doing this: nn.CrossEntropyLoss()(logits, torch.arange(batch_size)). The reason of not using that here is that the dataset we are using has multiple captions for a single image; so, there is the possibility that two identical images with their similar captions exist in a batch (it is rare but it can happen). Taking the loss with this easier method will ignore this possibility and the model learns to pull apart two representations (assume them different)  that are actually the same. Obviously, we don't want this to happen so I calculated the whole target matrix in a way that takes care of these edge cases.

## Train

Here are some funtions to help us load train and valid dataloaders, our model and then train and evaluate our model on those. There's not much going on here; just simple training loop and utility functions
"""

import pandas as pd
import numpy as np
import torch

def make_train_valid_dfs():
    """
    Splits the dataset into training and validation sets.

    Returns:
        train_dataframe (pd.DataFrame): Training dataset.
        valid_dataframe (pd.DataFrame): Validation dataset.
    """
    dataframe = pd.read_csv(cfg.captions_path)  # Fixed path issue
    max_id = dataframe["id"].max() + 1 if not cfg.debug else 100

    # Generate unique image IDs
    image_ids = np.arange(0, max_id)
    np.random.seed(42)

    # Select 20% of images for validation
    valid_ids = np.random.choice(image_ids, size=int(0.2 * len(image_ids)), replace=False)
    train_ids = np.setdiff1d(image_ids, valid_ids)

    # Create train and validation dataframes
    train_dataframe = dataframe[dataframe["id"].isin(train_ids)].reset_index(drop=True)
    valid_dataframe = dataframe[dataframe["id"].isin(valid_ids)].reset_index(drop=True)

    return train_dataframe, valid_dataframe


def build_loaders(dataframe, tokenizer, mode):
    """
    Builds DataLoader for training and validation.

    Args:
        dataframe (pd.DataFrame): Data containing image paths and captions.
        tokenizer (transformers.PreTrainedTokenizer): Tokenizer for text processing.
        mode (str): "train" or "valid" to define dataset mode.

    Returns:
        torch.utils.data.DataLoader: DataLoader for the given mode.
    """
    transforms = get_transforms(mode=mode)  # Apply EfficientNet-B3 compatible transforms

    dataset = CLIPDataset(
        dataframe["image"].values,
        dataframe["caption"].values,
        tokenizer=tokenizer,
        transforms=transforms,
    )

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,  # Ensure batch size is 32 (as defined in CFG)
        num_workers=cfg.num_workers,
        shuffle=(mode == "train"),  # Shuffle only during training
        pin_memory=True  # Enable for efficient GPU data loading
    )

    return dataloader

"""Here's a handy function to train our model. There's not much happening here; just loading the batches, feeding them to the model and stepping the optimizer and lr_scheduler."""

import torch
from tqdm import tqdm

def train_epoch(model, train_loader, optimizer, lr_scheduler, step):
    """
    Trains the model for one epoch.

    Args:
        model (nn.Module): The CLIP model.
        train_loader (DataLoader): Training data loader.
        optimizer (torch.optim.Optimizer): Optimizer for updating weights.
        lr_scheduler (torch.optim.lr_scheduler._LRScheduler): Learning rate scheduler.
        step (str): "batch" or "epoch" step for learning rate scheduling.

    Returns:
        AvgMeter: Average training loss.
    """
    model.train()  # Set model to training mode
    loss_meter = AvgMeter()
    tqdm_object = tqdm(train_loader, total=len(train_loader), desc="Training")

    for batch in tqdm_object:
        batch = {k: v.to(cfg.device) for k, v in batch.items() if k != "caption"}

        optimizer.zero_grad()
        loss = model(batch)  # Forward pass

        loss.backward()  # Backpropagation
        optimizer.step()  # Update weights

        if step == "batch":
            lr_scheduler.step()  # Adjust learning rate per batch

        # Track loss
        count = batch["image"].size(0)
        loss_meter.update(loss.item(), count)

        # Display progress
        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))

    return loss_meter


def valid_epoch(model, valid_loader):
    """
    Validates the model for one epoch.

    Args:
        model (nn.Module): The CLIP model.
        valid_loader (DataLoader): Validation data loader.

    Returns:
        AvgMeter: Average validation loss.
    """
    model.eval()  # Set model to evaluation mode
    loss_meter = AvgMeter()
    tqdm_object = tqdm(valid_loader, total=len(valid_loader), desc="Validation")

    with torch.no_grad():  # Disable gradient calculation for validation
        for batch in tqdm_object:
            batch = {k: v.to(cfg.device) for k, v in batch.items() if k != "caption"}

            loss = model(batch)  # Forward pass

            # Track loss
            count = batch["image"].size(0)
            loss_meter.update(loss.item(), count)

            # Display progress
            tqdm_object.set_postfix(valid_loss=loss_meter.avg)

    return loss_meter

"""Running the next cell start training the model. Put the kernel on GPU mode. Every epoch should take about 24 minutes on GPU (even one epoch is enough!)."""

# import torch
# import itertools
# from transformers import DistilBertTokenizer

# # Load DataFrames
# train_df, valid_df = make_train_valid_dfs()

# # Load Tokenizer
# tokenizer = DistilBertTokenizer.from_pretrained(cfg.text_tokenizer)

# # Create Data Loaders
# train_loader = build_loaders(train_df, tokenizer, mode="train")
# valid_loader = build_loaders(valid_df, tokenizer, mode="valid")

# # Initialize Model with EfficientNet-B3
# model = CLIPModel().to(cfg.device)

# # Optimizer Parameters (Separate LRs for Image Encoder, Text Encoder, and Projection Heads)
# params = [
#     {"params": model.image_encoder.parameters(), "lr": cfg.image_encoder_lr},
#     {"params": model.text_encoder.parameters(), "lr": cfg.text_encoder_lr},
#     {"params": itertools.chain(
#         model.image_projection.parameters(), model.text_projection.parameters()
#     ), "lr": cfg.head_lr, "weight_decay": cfg.weight_decay}
# ]

# # AdamW Optimizer
# optimizer = torch.optim.AdamW(params, weight_decay=cfg.weight_decay)

# # Learning Rate Scheduler (Reduce LR on Plateau)
# lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
#     optimizer, mode="min", patience=cfg.patience, factor=cfg.factor
# )

# # Early Stopping Setup
# best_loss = float('inf')
# early_stopping_counter = 0
# early_stopping_patience = cfg.early_stopping_patience  # Define in config

# # Training Loop
# for epoch in range(cfg.epochs):
#     print(f"\nð¹ Epoch {epoch + 1}/{cfg.epochs}")

#     # Train Model
#     model.train()
#     train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step="epoch")

#     # Validate Model
#     model.eval()
#     with torch.no_grad():
#         valid_loss = valid_epoch(model, valid_loader)

#     # Check if validation loss improved
#     if valid_loss.avg < best_loss:
#         best_loss = valid_loss.avg
#         early_stopping_counter = 0  # Reset counter
#         torch.save(model.state_dict(), "best.pt")
#         print("Saved Best Model!")
#     else:
#         early_stopping_counter += 1  # Increment counter

#     # Step LR Scheduler
#     lr_scheduler.step(valid_loss.avg)

#     # Early Stopping Condition
#     if early_stopping_counter >= early_stopping_patience:
#         print(f" Early stopping triggered! No improvement for {early_stopping_patience} epochs.")
#         break  # Stop training






import torch
import itertools
from transformers import DistilBertTokenizer

# Load DataFrames
train_df, valid_df = make_train_valid_dfs()

# Load Tokenizer
tokenizer = DistilBertTokenizer.from_pretrained(cfg.text_tokenizer)

# Create Data Loaders
train_loader = build_loaders(train_df, tokenizer, mode="train")
valid_loader = build_loaders(valid_df, tokenizer, mode="valid")

# Initialize Model with EfficientNet-B3
model = CLIPModel().to(cfg.device)

# Optimizer Parameters (Separate LRs for Image Encoder, Text Encoder, and Projection Heads)
params = [
    {"params": model.image_encoder.parameters(), "lr": cfg.image_encoder_lr},
    {"params": model.text_encoder.parameters(), "lr": cfg.text_encoder_lr},
    {"params": itertools.chain(
        model.image_projection.parameters(), model.text_projection.parameters()
    ), "lr": cfg.head_lr, "weight_decay": cfg.weight_decay}
]

# AdamW Optimizer
optimizer = torch.optim.AdamW(params, weight_decay=cfg.weight_decay)

# Learning Rate Scheduler (Reduce LR on Plateau)
lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode="min", patience=cfg.patience, factor=cfg.factor
)

# Early Stopping Setup
best_loss = float('inf')
early_stopping_counter = 0
early_stopping_patience = cfg.early_stopping_patience  # Define in config

# Training Loop
for epoch in range(cfg.epochs):
    print(f"\nð¹ Epoch {epoch + 1}/{cfg.epochs}")

    # Train Model
    model.train()
    train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step="epoch")

    # Validate Model
    model.eval()
    with torch.no_grad():
        valid_loss = valid_epoch(model, valid_loader)

    # Print Losses
    print(f"ð Training Loss: {train_loss.avg:.4f} | ð Validation Loss: {valid_loss.avg:.4f}")

    # Check if validation loss improved
    if valid_loss.avg < best_loss:
        best_loss = valid_loss.avg
        early_stopping_counter = 0  # Reset counter
        torch.save(model.state_dict(), "best.pt")
        print("â Saved Best Model!")
    else:
        early_stopping_counter += 1  # Increment counter

    # Step LR Scheduler
    lr_scheduler.step(valid_loss.avg)

    # Early Stopping Condition
    if early_stopping_counter >= early_stopping_patience:
        print(f"â¹ï¸ Early stopping triggered! No improvement for {early_stopping_patience} epochs.")
        break  # Stop training

import torch
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
from transformers import DistilBertTokenizer

# Load trained model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CLIPModel().to(device)
model.load_state_dict(torch.load("/kaggle/input/dataset1/best (1).pt", map_location=device))
model.eval()

# Initialize tokenizer
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")

def extract_features(dataloader, model, feature_type="image"):
    """Extract normalized image or text features from the model."""
    features = []
    with torch.no_grad():
        for batch in tqdm(dataloader, desc=f"Extracting {feature_type} features"):
            batch = {k: v.to(device) for k, v in batch.items() if k != "caption"}

            if feature_type == "image":
                feature = model.image_projection(model.image_encoder(batch["image"]))
            else:
                feature = model.text_projection(model.text_encoder(
                    input_ids=batch["input_ids"], attention_mask=batch["attention_mask"]
                ))

            features.append(F.normalize(feature, dim=1).cpu().numpy())
    return np.vstack(features)

# Load validation dataset
_, valid_dataframe = make_train_valid_dfs()
valid_loader = build_loaders(valid_dataframe, tokenizer, mode="valid")

# Extract image and text features
image_features = extract_features(valid_loader, model, "image")
text_features = extract_features(valid_loader, model, "text")

# Compute similarity matrix
similarity_matrix = np.matmul(text_features, image_features.T)

def compute_retrieval_metrics(similarity_matrix, valid_df, top_k=[1, 5, 10]):
    """Compute Recall@K and Mean Average Precision (mAP)."""
    num_queries = similarity_matrix.shape[0]
    recall_at_k = {k: 0 for k in top_k}
    average_precision = []

    image_to_indices = valid_df.groupby("image").indices

    for i in range(num_queries):
        sorted_indices = np.argsort(similarity_matrix[i])[::-1]
        gt_image = valid_df.iloc[i]["image"]
        gt_indices = image_to_indices[gt_image]

        for k in top_k:
            if any(idx in sorted_indices[:k] for idx in gt_indices):
                recall_at_k[k] += 1

        ranks = [np.where(sorted_indices == idx)[0][0] + 1 for idx in gt_indices]
        average_precision.append(np.mean([1.0 / rank for rank in ranks]))

    recall_at_k = {k: recall_at_k[k] / num_queries for k in recall_at_k}
    mean_ap = np.mean(average_precision)
    return recall_at_k, mean_ap

# Compute and display metrics
recall_k, mean_ap = compute_retrieval_metrics(similarity_matrix, valid_dataframe)
print("\nRetrieval Performance:")
for k in recall_k:
    print(f"Recall@{k}: {recall_k[k]:.4f}")
print(f"Mean Average Precision (mAP): {mean_ap:.4f}")

"""## Inference

Okay! We are done with training the model. Now, we need to do inference which in our case will be giving the model a piece of text and want it to retrieve the most relevant images from an unseen validation (or test) set.

### Getting Image Embeddings

In this function, we are loading the model that we saved after training, feeding it images in validation set and returning the image_embeddings with shape (valid_set_size, 256) and the model itself.
"""

def get_image_embeddings(valid_df, model_path):
    tokenizer = DistilBertTokenizer.from_pretrained(cfg.text_tokenizer)
    valid_loader = build_loaders(valid_df, tokenizer, mode="valid")

    model = CLIPModel().to(cfg.device)
    model.load_state_dict(torch.load(model_path, map_location=cfg.device))
    model.eval()

    valid_image_embeddings = []
    with torch.no_grad():
        for batch in tqdm(valid_loader):
            image_features = model.image_encoder(batch["image"].to(cfg.device))
            image_embeddings = model.image_projection(image_features)
            valid_image_embeddings.append(image_embeddings)
    return model, torch.cat(valid_image_embeddings)

_, valid_df = make_train_valid_dfs()
model, image_embeddings = get_image_embeddings(valid_df, "/kaggle/input/dataset1/best (1).pt")

"""### Finding Matches

This function does the final task that we wished our model would be capable of: it gets the model, image_embeddings, and a text query. It will display the most relevant images from the validation set! Isn't it amazing? Let's see how it performs.
"""

def find_matches(model, image_embeddings, query, image_filenames, n=9):
    tokenizer = DistilBertTokenizer.from_pretrained(cfg.text_tokenizer)
    encoded_query = tokenizer([query])
    batch = {
        key: torch.tensor(values).to(cfg.device)
        for key, values in encoded_query.items()
    }
    with torch.no_grad():
        text_features = model.text_encoder(
            input_ids=batch["input_ids"], attention_mask=batch["attention_mask"]
        )
        text_embeddings = model.text_projection(text_features)

    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)
    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)
    dot_similarity = text_embeddings_n @ image_embeddings_n.T

    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)
    matches = [image_filenames[idx] for idx in indices[::5]]

    _, axes = plt.subplots(3, 3, figsize=(10, 10))
    for match, ax in zip(matches, axes.flatten()):
        image = cv2.imread(f"{cfg.image_path}/{match}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        ax.imshow(image)
        ax.axis("off")

    plt.show()

"""This is how we use this function. The results:"""

find_matches(model,
             image_embeddings,
             query="one dog sitting on the grass",
             image_filenames=valid_df['image'].values,
             n=9)

import time
# Inference timing example
start_time = time.time()
with torch.no_grad():
    find_matches(model,
             image_embeddings,
             query="Two young guys with shaggy hair look at their hands while hanging out in the yard",
             image_filenames=valid_df['image'].values,
             n=9)  # Replace `batch` with your actual inference input
end_time = time.time()
print(f"ð Inference Time: {end_time - start_time:.4f} seconds")

!pip install thop

# Make sure thop is installed
# !pip install thop
from thop import profile, clever_format

# Dummy input matching the model signature
dummy_image = torch.randn(1, 3, cfg.size, cfg.size).to(cfg.device)
dummy_input_ids = torch.randint(0, 30522, (1, cfg.max_length)).to(cfg.device)
dummy_attention_mask = torch.ones((1, cfg.max_length)).to(cfg.device)

dummy_batch = {
    "image": dummy_image,
    "input_ids": dummy_input_ids,
    "attention_mask": dummy_attention_mask
}

# Calculate FLOPs and Params
flops, params = profile(model, inputs=(dummy_batch,), verbose=False)
flops, params = clever_format([flops, params], "%.3f")
print(f"ð¢ FLOPs: {flops} | ð¦ Parameters: {params}")

import time
queries = [
    "A dog sitting on the grass",
    "A man riding a bicycle",
    "A woman walking in the park",
    "Two kids playing soccer",
    "A cat sleeping on a sofa"
]

start_time = time.time()
with torch.no_grad():
    for q in queries:
        find_matches(
            model,
            image_embeddings,
            query=q,
            image_filenames=valid_df['image'].values,
            n=9
        )
end_time = time.time()
print(f"ð Total Inference Time for {len(queries)} queries: {end_time - start_time:.4f} seconds")
print(f"â±ï¸ Average per query: {(end_time - start_time) / len(queries):.4f} seconds")

