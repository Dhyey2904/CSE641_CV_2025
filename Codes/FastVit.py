# -*- coding: utf-8 -*-
"""fork-of-v3-small.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qULwRnjbdj7fFLmxW69WB8K7fxb0Lz7V

# CLIP from Scratch


**CLIP** or **Contrastive Language-Image Pre-training** is a model that learns the relationship between a whole sentence and the image it describes; in a sense that when the model is trained, given an input sentence it will be able to retrieve the most related images corresponding to that sentence. The important thing here is that it is trained on full sentences instead of single classes like car, dog, etc. The intuition is that when trained on whole sentences, the model can learn a lot more things and finds some pattern between images and texts.
They also show that when this model is trained on a huge dataset of images and their corresponding texts, it can also act as a classifier too. I encourage you to study the paper to learn more about this exciting model and their astonishing results on benchmarking datasetsÂ . To mention just one, CLIP model trained with this strategy classifies ImageNet better than those SOTA models trained on the ImageNet itself optimized for the only task of classification!

As a **teaser**, let's see what the final model that we will build in this article from scratch is capable of: given a query (raw text) like "a boy jumping with skateboard" or "a girl jumping from swing", the model will retrieve the most relevant images:

![](https://i.ibb.co/9gdYqNP/teaser-cropped.png)

In this notebook, we will see how to implement CLIP from Scratch
"""

import kagglehub
hsankesara_flickr_image_dataset_path = kagglehub.dataset_download('hsankesara/flickr-image-dataset')

print('Data source import complete.')
print(hsankesara_flickr_image_dataset_path)

"""## Imports"""

!pip install -q timm

# -*- coding: utf-8 -*-
"""Optimized CLIP with MobileNetV2 - Fixed Checkpointing"""

import os
import cv2
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import albumentations as A
from albumentations.pytorch import ToTensorV2
import matplotlib.pyplot as plt
from transformers import DistilBertModel, DistilBertTokenizer, DistilBertConfig
from torch.cuda.amp import autocast, GradScaler
from torch.utils.checkpoint import checkpoint  # Gradient checkpointing
import os
import timm

"""## Config"""

df = pd.read_csv("/kaggle/input/rstp-new/RSTPReid/reformatted_captions.csv")
len(df)

# Config
class CFG:
    debug = False
    image_path = "/kaggle/input/rstp-new/RSTPReid/imgs"
    captions_path = "/kaggle/input/rstp-new/RSTPReid"
    batch_size = 32
    num_workers = 4
    head_lr = 1e-3
    image_encoder_lr = 1e-4
    text_encoder_lr = 1e-5
    weight_decay = 1e-3
    patience = 1
    factor = 0.8
    epochs = 25
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model_name = 'fastvit_t8'
    image_embedding = 512
    text_encoder_model = "distilbert-base-uncased"
    text_embedding = 768
    text_tokenizer = "distilbert-base-uncased"
    max_length = 200
    pretrained = True
    trainable = True
    temperature = 1.0  # Initial value, will be replaced by learnable logit_scale
    size = 224
    num_projection_layers = 1
    projection_dim = 256
    dropout = 0.1

cfg = CFG()

# Utils
class AvgMeter:
    def __init__(self, name="Metric"):
        self.name = name
        self.reset()

    def reset(self):
        self.avg, self.sum, self.count = [0] * 3

    def update(self, val, count=1):
        self.count += count
        self.sum += val * count
        self.avg = self.sum / self.count

    def __repr__(self):
        text = f"{self.name}: {self.avg:.4f}"
        return text

def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group["lr"]

# Dataset
class CLIPDataset(torch.utils.data.Dataset):
    def __init__(self, image_filenames, captions, tokenizer, transforms):
        self.image_filenames = image_filenames
        self.captions = list(captions)
        self.encoded_captions = tokenizer(
            list(captions), padding=True, truncation=True, max_length=cfg.max_length
        )
        self.transforms = transforms

    def __getitem__(self, idx):
        item = {
            key: torch.tensor(values[idx])
            for key, values in self.encoded_captions.items()
        }
        image = cv2.imread(f"{cfg.image_path}/{self.image_filenames[idx]}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = self.transforms(image=image)['image']
        item['image'] = torch.tensor(image).permute(2, 0, 1).float()
        item['caption'] = self.captions[idx]
        return item

    def __len__(self):
        return len(self.captions)

def get_transforms(mode="train"):
    if mode == "train":
        return A.Compose([
            A.Resize(cfg.size, cfg.size, always_apply=True),
            A.Normalize(max_pixel_value=255.0, always_apply=True),
        ])
    else:
        return A.Compose([
            A.Resize(cfg.size, cfg.size, always_apply=True),
            A.Normalize(max_pixel_value=255.0, always_apply=True),
        ])

# Optimized Image Encoder with Attention Pooling
class AttentionPool2d(nn.Module):
    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):
        super().__init__()
        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)
        self.num_heads = num_heads

    def forward(self, x):
        x = x.flatten(start_dim=2).permute(2, 0, 1)  # NCHW -> (HW)NC
        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC
        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC
        x, _ = F.multi_head_attention_forward(
            query=x[:1], key=x, value=x,
            embed_dim_to_check=x.shape[-1],
            num_heads=self.num_heads,
            q_proj_weight=self.q_proj.weight,
            k_proj_weight=self.k_proj.weight,
            v_proj_weight=self.v_proj.weight,
            in_proj_weight=None,
            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),
            bias_k=None,
            bias_v=None,
            add_zero_attn=False,
            dropout_p=0,
            out_proj_weight=self.c_proj.weight,
            out_proj_bias=self.c_proj.bias,
            use_separate_proj_weight=True,
            training=self.training,
            need_weights=False
        )
        return x.squeeze(0)

class ImageEncoder(nn.Module):
    def __init__(self, embed_dim=cfg.image_embedding):
        super(ImageEncoder, self).__init__()
        self.backbone = timm.create_model("fastvit_t8", pretrained=cfg.pretrained, features_only=True)
        self.out_channels = self.backbone.feature_info[-1]['num_chs']  # should be 256 for t8
        self.attnpool = AttentionPool2d(spacial_dim=7, embed_dim=self.out_channels, num_heads=8, output_dim=embed_dim)

        std = self.out_channels ** -0.5
        nn.init.normal_(self.attnpool.q_proj.weight, std=std)
        nn.init.normal_(self.attnpool.k_proj.weight, std=std)
        nn.init.normal_(self.attnpool.v_proj.weight, std=std)
        nn.init.normal_(self.attnpool.c_proj.weight, std=std)

    def forward(self, x):
        features = self.backbone(x)  # List of feature maps
        x = features[-1]  # Take the last (deepest) feature map
        x = self.attnpool(x)
        return x


# Optimized Text Encoder with Final LayerNorm
class LayerNorm(nn.LayerNorm):
    def forward(self, x: torch.Tensor):
        orig_type = x.dtype
        ret = super().forward(x.type(torch.float32))
        return ret.type(orig_type)

class TextEncoder(nn.Module):
    def __init__(self, model_name=cfg.text_encoder_model, pretrained=cfg.pretrained, trainable=cfg.trainable):
        super().__init__()
        if pretrained:
            self.model = DistilBertModel.from_pretrained(model_name)
        else:
            self.model = DistilBertModel(config=DistilBertConfig())
        for p in self.model.parameters():
            p.requires_grad = trainable
        self.target_token_idx = 0
        self.ln_final = LayerNorm(cfg.text_embedding)

    def forward(self, input_ids, attention_mask):
        output = self.model(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden_state = output.last_hidden_state
        x = last_hidden_state[:, self.target_token_idx, :]
        x = self.ln_final(x)
        return x

# Optimized Projection Head with Residual Connection
class ProjectionHead(nn.Module):
    def __init__(self, embedding_dim, projection_dim=cfg.projection_dim, dropout=cfg.dropout):
        super().__init__()
        self.projection = nn.Linear(embedding_dim, projection_dim)
        self.gelu = nn.GELU()
        self.fc = nn.Linear(projection_dim, projection_dim)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(projection_dim)
        proj_std = (embedding_dim ** -0.5) * ((2 * cfg.num_projection_layers) ** -0.5)
        nn.init.normal_(self.projection.weight, std=proj_std)
        nn.init.normal_(self.fc.weight, std=proj_std)

    def forward(self, x):
        projected = self.projection(x)
        x = self.gelu(projected)
        x = self.fc(x)
        x = self.dropout(x)
        x = x + projected
        x = self.layer_norm(x)
        return x

# Optimized CLIP Model with Normalized Features and Logit Scale
class CLIPModel(nn.Module):
    def __init__(self, temperature=cfg.temperature, image_embedding=cfg.image_embedding, text_embedding=cfg.text_embedding):
        super().__init__()
        self.image_encoder = ImageEncoder()
        self.text_encoder = TextEncoder()
        self.image_projection = ProjectionHead(embedding_dim=image_embedding)
        self.text_projection = ProjectionHead(embedding_dim=text_embedding)
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

    def forward(self, batch):
        image_features = self.image_encoder(batch["image"])
        text_features = self.text_encoder(input_ids=batch["input_ids"], attention_mask=batch["attention_mask"])
        image_embeddings = self.image_projection(image_features)
        text_embeddings = self.text_projection(text_features)
        image_embeddings = F.normalize(image_embeddings, p=2, dim=-1)
        text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)
        logit_scale = self.logit_scale.exp()
        logits = logit_scale * (text_embeddings @ image_embeddings.T)
        images_similarity = image_embeddings @ image_embeddings.T
        texts_similarity = text_embeddings @ text_embeddings.T
        targets = F.softmax((images_similarity + texts_similarity) / 2 * logit_scale, dim=-1)
        texts_loss = cross_entropy(logits, targets, reduction='none')
        images_loss = cross_entropy(logits.T, targets.T, reduction='none')
        loss = (images_loss + texts_loss) / 2.0
        return loss.mean()

def cross_entropy(preds, targets, reduction='none'):
    log_softmax = nn.LogSoftmax(dim=-1)
    loss = (-targets * log_softmax(preds)).sum(1)
    if reduction == "none":
        return loss
    elif reduction == "mean":
        return loss.mean()

# Training Functions
def make_train_valid_dfs():
    dataframe = pd.read_csv(f"{cfg.captions_path}/reformatted_captions.csv")
    if cfg.debug:
        dataframe = dataframe.sample(100, random_state=42).reset_index(drop=True)
    else:
        dataframe = dataframe.reset_index(drop=True)
    indices = np.arange(len(dataframe))
    np.random.seed(42)
    valid_indices = np.random.choice(indices, size=int(0.2 * len(indices)), replace=False)
    train_indices = [i for i in indices if i not in valid_indices]
    train_dataframe = dataframe.iloc[train_indices].reset_index(drop=True)
    valid_dataframe = dataframe.iloc[valid_indices].reset_index(drop=True)
    return train_dataframe, valid_dataframe


def build_loaders(dataframe, tokenizer, mode):
    transforms = get_transforms(mode=mode)
    dataset = CLIPDataset(dataframe["image"].values, dataframe["caption"].values, tokenizer, transforms)
    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=cfg.batch_size, num_workers=cfg.num_workers, shuffle=True if mode == "train" else False
    )
    return dataloader

def train_epoch(model, train_loader, optimizer, lr_scheduler, step):
    loss_meter = AvgMeter()
    tqdm_object = tqdm(train_loader, total=len(train_loader))
    for batch in tqdm_object:
        batch = {k: v.to(cfg.device) for k, v in batch.items() if k != "caption"}
        loss = model(batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if step == "batch":
            lr_scheduler.step()
        count = batch["image"].size(0)
        loss_meter.update(loss.item(), count)
        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))
    return loss_meter

def valid_epoch(model, valid_loader):
    loss_meter = AvgMeter()
    tqdm_object = tqdm(valid_loader, total=len(valid_loader))
    for batch in tqdm_object:
        batch = {k: v.to(cfg.device) for k, v in batch.items() if k != "caption"}
        loss = model(batch)
        count = batch["image"].size(0)
        loss_meter.update(loss.item(), count)
        tqdm_object.set_postfix(valid_loss=loss_meter.avg)
    return loss_meter

import itertools
train_df, valid_df = make_train_valid_dfs()
tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)
train_loader = build_loaders(train_df, tokenizer, mode="train")
valid_loader = build_loaders(valid_df, tokenizer, mode="valid")

model = CLIPModel().to(CFG.device)
params = [
    {"params": model.image_encoder.parameters(), "lr": cfg.image_encoder_lr},
    {"params": model.text_encoder.parameters(), "lr": cfg.text_encoder_lr},
    {"params": itertools.chain(model.image_projection.parameters(), model.text_projection.parameters()),
     "lr": cfg.head_lr, "weight_decay": cfg.weight_decay}
]
optimizer = torch.optim.AdamW(params, weight_decay=0.)
lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode="min", patience=cfg.patience, factor=cfg.factor)
step = "epoch"

best_loss = float('inf')
for epoch in range(cfg.epochs):
    print(f"Epoch: {epoch + 1}")
    model.train()
    train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)
    model.eval()
    with torch.no_grad():
        valid_loss = valid_epoch(model, valid_loader)
    if valid_loss.avg < best_loss:
        best_loss = valid_loss.avg
        torch.save(model.state_dict(), "best.pt")
        print("Saved Best Model!")
    lr_scheduler.step(valid_loss.avg)

import torch
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm

# Load trained model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Step 1: Initialize the CLIP Model
model = CLIPModel().to(device)  # Make sure this matches your training architecture

# Step 2: Load the trained weights into the model
model.load_state_dict(torch.load("/kaggle/working/best.pt", map_location=device))  # Load state_dict

# Step 3: Set the model to evaluation mode
model.eval()  # Load the best saved model
tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)
# # Function to extract features
def extract_features(dataloader, model, feature_type="image"):
    """Extract image or text features using the trained model."""
    features = []
    with torch.no_grad():
        for batch in tqdm(dataloader, desc=f"Extracting {feature_type} features"):
            batch = {k: v.to(device) for k, v in batch.items() if k != "caption"}

            if feature_type == "image":
                feature = model.image_projection(model.image_encoder(batch["image"]))
            else:  # Text Features
                feature = model.text_projection(model.text_encoder(
                    input_ids=batch["input_ids"], attention_mask=batch["attention_mask"]
                ))

            feature = F.normalize(feature, dim=1)  # Normalize for cosine similarity
            features.append(feature.cpu().numpy())

    return np.vstack(features)

# Load validation dataset
_, valid_dataframe = make_train_valid_dfs()  # Get validation split

# Initialize tokenizer (assuming you already have a tokenizer object)
valid_loader = build_loaders(valid_dataframe, tokenizer, mode="valid")

# Extract features using validation data
image_features = extract_features(valid_loader, model, "image")
text_features = extract_features(valid_loader, model, "text")


# Compute cosine similarity between image and text features
similarity_matrix = np.matmul(text_features, image_features.T)

# # Function to compute retrieval metrics
def compute_rank_and_map(similarity_matrix, top_k=[1, 5, 10]):
    num_queries = similarity_matrix.shape[0]
    rank_counts = {k: 0 for k in top_k}
    reciprocal_ranks = []

    for i in range(num_queries):
        sorted_indices = np.argsort(similarity_matrix[i])[::-1]  # Highest similarity first
        gt_index = i  # Assuming ground truth is at the same index
        rank = np.where(sorted_indices == gt_index)[0][0] + 1  # 1-based rank

        reciprocal_ranks.append(1.0 / rank)

        # Count how often correct match is exactly at position K
        for k in top_k:
            if rank == k:
                rank_counts[k] += 1

    # Normalize to get Rank@K as a percentage
    rank_at_k = {k: rank_counts[k] / num_queries for k in rank_counts}
    mean_ap = np.mean(reciprocal_ranks)

    return rank_at_k, mean_ap


# Compute metrics
rank_k, mean_ap = compute_rank_and_map(similarity_matrix)

# Print results
print("\nRetrieval Performance:")
for k in rank_k:
    print(f"Rank@{k}: {rank_k[k]:.4f}")
print(f"Mean Average Precision (mAP): {mean_ap:.4f}")

"""## Inference

Okay! We are done with training the model. Now, we need to do inference which in our case will be giving the model a piece of text and want it to retrieve the most relevant images from an unseen validation (or test) set.

### Getting Image Embeddings

In this function, we are loading the model that we saved after training, feeding it images in validation set and returning the image_embeddings with shape (valid_set_size, 256) and the model itself.
"""

def get_image_embeddings(valid_df, model_path):
    tokenizer = DistilBertTokenizer.from_pretrained(cfg.text_tokenizer)
    valid_loader = build_loaders(valid_df, tokenizer, mode="valid")

    model = CLIPModel().to(cfg.device)
    model.load_state_dict(torch.load(model_path, map_location=cfg.device))
    model.eval()

    valid_image_embeddings = []
    with torch.no_grad():
        for batch in tqdm(valid_loader):
            image_features = model.image_encoder(batch["image"].to(cfg.device))
            image_embeddings = model.image_projection(image_features)
            valid_image_embeddings.append(image_embeddings)
    return model, torch.cat(valid_image_embeddings)

_, valid_df = make_train_valid_dfs()
model, image_embeddings = get_image_embeddings(valid_df, "/kaggle/working/best.pt")

"""### Finding Matches

This function does the final task that we wished our model would be capable of: it gets the model, image_embeddings, and a text query. It will display the most relevant images from the validation set! Isn't it amazing? Let's see how it performs.
"""

def find_matches(model, image_embeddings, query, image_filenames, n=9):
    tokenizer = DistilBertTokenizer.from_pretrained(cfg.text_tokenizer)
    encoded_query = tokenizer([query])
    batch = {
        key: torch.tensor(values).to(cfg.device)
        for key, values in encoded_query.items()
    }
    with torch.no_grad():
        text_features = model.text_encoder(
            input_ids=batch["input_ids"], attention_mask=batch["attention_mask"]
        )
        text_embeddings = model.text_projection(text_features)

    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)
    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)
    dot_similarity = text_embeddings_n @ image_embeddings_n.T

    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)
    matches = [image_filenames[idx] for idx in indices[::5]]

    _, axes = plt.subplots(3, 3, figsize=(10, 10))
    for match, ax in zip(matches, axes.flatten()):
        image = cv2.imread(f"{cfg.image_path}/{match}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        ax.imshow(image)
        ax.axis("off")

    plt.show()

"""This is how we use this function. The results:"""

find_matches(model,
             image_embeddings,
             query="one dog sitting on the grass",
             image_filenames=valid_df['image'].values,
             n=9)